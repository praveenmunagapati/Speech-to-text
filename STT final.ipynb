{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xUDHSxTFwD96"
   },
   "source": [
    "# A comparitive study of various most prominent alternatives for Speech to text \n",
    "  - Confined to batch data. \n",
    "  - Evaluation on the Word Error rate on the cleaned audio files from [Librispeech](http://www.openslr.org/12/)\n",
    "  -  Aternatives comapred:\n",
    "    - Mozilla DeepSpeech\n",
    "    - Google Speech-to-Text\n",
    "    - Microsoft Azure Speech Cloud Service "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NhwbzJRt1F9F"
   },
   "source": [
    "Step 0 - Defining the various functions to:\n",
    "  - to converting flac files to wav\n",
    "  - mount google drive \n",
    "  - reading the information from the labels\n",
    "  - reading the wav files to get rate and buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages required\n",
    "\n",
    "# !pip install deepspeech\n",
    "# !pip install google-cloud-speech\n",
    "#!pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JcbjfIMBwDKH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sox is already the newest version (14.4.1-5+deb9u2).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "# Importing and installing packages\n",
    "\n",
    "import subprocess\n",
    "import re\n",
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import unicodedata\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "from numpy import floor\n",
    "import time\n",
    "\n",
    "import wave\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from jiwer import wer\n",
    "\n",
    "\n",
    "!sudo apt -qq install -y sox\n",
    "\n",
    "\n",
    "#--------------------------------------------------------\n",
    "#FYI - \n",
    "# The librispeech file corpus comes in various subfolders and you may want to \n",
    "# move them to one folder. \n",
    "#Cmd for moving all files from all subfolders to the main folder\n",
    "# for /r %d in (*) do copy \"%d\" \"D:\\All Essec\\CRP\\DeepSpeech\\LibriSpeech\\All\" \n",
    "#--------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eUxbdRAFACIO"
   },
   "outputs": [],
   "source": [
    "# function to convert all flac file in a directory to wav \n",
    "\n",
    "# defining function to replace in a string from left\n",
    "def rreplace(pattern, sub, string):\n",
    "    \"\"\"\n",
    "    Replaces 'pattern' in 'string' with 'sub' if 'pattern' ends 'string'.\n",
    "    \"\"\"\n",
    "    return re.sub('%s$' % pattern, sub, string)\n",
    "\n",
    "\n",
    "\n",
    "# defining function to convert flac to wav\n",
    "def flac_to_wav(inp, dest):\n",
    "    \n",
    "   \n",
    "   #inp - directory to the folder where the flac files are stored\n",
    "   #dest -directory to the folder where the flac files will be moved\n",
    "\n",
    "   #note - Dest will be deleted eventually\n",
    "  print(inp)\n",
    "  for file in os.listdir(inp):\n",
    "      if file.endswith('.flac'):\n",
    "        current_file = os.path.join(inp, file)\n",
    "        out = rreplace('flac','wav',current_file)\n",
    "        subprocess.run([\"sox\", current_file, \"-c 1\", \"-r 16000\", out])\n",
    "#         print('File Name:', current_file)\n",
    "\n",
    "\n",
    "  print('All files converted done')\n",
    "\n",
    "  # move all the flac files to a new folder\n",
    "  source = pathlib.Path(inp)\n",
    "  dest1 = pathlib.Path(dest)\n",
    "  dest1.mkdir(exist_ok=True)\n",
    "\n",
    "  for f in source.glob(\"*.flac\"):\n",
    "      shutil.move(f, dest1.joinpath(f.name))\n",
    "#       print(f, 'moved')\n",
    "\n",
    "  print('All files moved to trash folder')\n",
    "  # # delete the folder created with the redundant flac files\n",
    "  shutil.rmtree(dest1)\n",
    "  print('trash folder deleted successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1549,
     "status": "ok",
     "timestamp": 1583239511890,
     "user": {
      "displayName": "Raghav VASHISHT",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgImMGLPwQlOQhdOWwSroULfrjFyUzyb9sGv0Wp=s64",
      "userId": "03595932606881468349"
     },
     "user_tz": -60
    },
    "id": "mkZa5sqT8J-4",
    "outputId": "0d349720-8ddb-4be9-c0a2-7dc193403dda"
   },
   "outputs": [],
   "source": [
    "# example on how to run flac_to_wav:\n",
    "\n",
    "inp2 = \"/home/jupyter/CRP/Data/audio/Cleaned Audio files\"\n",
    "dest2 = \"/home/jupyter/CRP/Data/audio/Cleaned Audio files/toDelete\"\n",
    "flac_to_wav(inp2, dest2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pC3-qyUt9XvD"
   },
   "outputs": [],
   "source": [
    "# read all label files and make a dataframe\n",
    "# gives output as a dataframe\n",
    "\n",
    "def read_from_label(path):\n",
    "  \n",
    "  trans = []\n",
    "  file_name = []\n",
    "\n",
    "  for file in os.listdir(path):\n",
    "      current_file = os.path.join(path, file)\n",
    "\n",
    "      with codecs.open(current_file, \"r\", \"utf-8\") as fin:\n",
    "        for line in fin:\n",
    "          # Parse each segment line\n",
    "          first_space = line.find(\" \")\n",
    "          seqid, transcript = line[:first_space], line[first_space+1:]\n",
    "\n",
    "          #----------------Optional -------------------------\n",
    "          # We need to do the encode-decode dance here because encode\n",
    "          # returns a bytes() object on Python 3, and text_to_char_array\n",
    "          # expects a string.\n",
    "          # transcript = unicodedata.normalize(\"NFKD\", transcript)  \\\n",
    "          #                         .encode(\"ascii\", \"ignore\")      \\\n",
    "          #                         .decode(\"ascii\", \"ignore\")\n",
    "          #-----------------------------------------------------------\n",
    "          transcript = transcript.lower().strip()\n",
    "\n",
    "          trans.append(transcript)\n",
    "          file_name.append(seqid)\n",
    "          \n",
    "        label  = pd.DataFrame(data=trans, columns=[ \"transcript\"])\n",
    "        files = pd.DataFrame(data=file_name, columns=[\"file\"])\n",
    "\n",
    "  labelled_data = label.join(files)\n",
    "  return labelled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BbZoYBfZEvzF"
   },
   "outputs": [],
   "source": [
    "# Example on how to create a data frame from the labelled translation\n",
    "path = \"/home/jupyter/CRP/Data/audio/translation\"\n",
    "labelled = read_from_label(path)\n",
    "labelled.to_csv('labelled.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GFyjOeN7IGMR"
   },
   "outputs": [],
   "source": [
    "# function to read the wav files to get rate and buffer\n",
    "\n",
    "def read_wav_file(filename) -> Tuple[bytes, int]:\n",
    "    with wave.open(filename, 'rb') as w:\n",
    "        rate = w.getframerate()\n",
    "        frames = w.getnframes()\n",
    "        buffer = w.readframes(frames)\n",
    "\n",
    "    return buffer, rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1qkEJAudJnGp"
   },
   "source": [
    "# 1. Mozilla DeepSpeech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 39787,
     "status": "ok",
     "timestamp": 1583236360802,
     "user": {
      "displayName": "Raghav VASHISHT",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgImMGLPwQlOQhdOWwSroULfrjFyUzyb9sGv0Wp=s64",
      "userId": "03595932606881468349"
     },
     "user_tz": -60
    },
    "id": "Ilr69IFcJsc1",
    "outputId": "6206b0e5-3949-48b4-9ad1-b9fb2581b97f"
   },
   "outputs": [],
   "source": [
    "#installing DeepSpeech\n",
    "\n",
    "!pip install deepspeech==0.6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepspeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the model and unzipping it \n",
    "!curl -LO https://github.com/mozilla/DeepSpeech/releases/download/v0.6.1/deepspeech-0.6.1-models.tar.gz\n",
    "!tar -xvzf deepspeech-0.6.1-models.tar.gz\n",
    "!ls -l ./deepspeech-0.6.1-models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the deepspeech model\n",
    "model_file_path = 'deepspeech-0.6.1-models/output_graph.pbmm'\n",
    "beam_width = 100\n",
    "model = deepspeech.Model(model_file_path, beam_width)\n",
    "\n",
    "lm_file_path = 'deepspeech-0.6.1-models/lm.binary'\n",
    "trie_file_path = 'deepspeech-0.6.1-models/trie'\n",
    "lm_alpha = 0.75\n",
    "lm_beta = 1.85\n",
    "model.enableDecoderWithLM(lm_file_path, trie_file_path, lm_alpha, lm_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aRjw_i7RN7XH"
   },
   "outputs": [],
   "source": [
    "#Defining the function for the model prediction\n",
    "\n",
    "\n",
    "def Deep_Speech_stt(inp):\n",
    "\n",
    "    \n",
    "  file_list = []\n",
    "  text_list = []\n",
    "\n",
    "\n",
    "  for file in os.listdir(inp):\n",
    "    try:\n",
    "      current_file = os.path.join(inp, file)\n",
    "      w = wave.open(current_file, 'r')\n",
    "      rate = w.getframerate()\n",
    "      frames = w.getnframes()\n",
    "      buffer = w.readframes(frames)\n",
    "      data16 = np.frombuffer(buffer, dtype=np.int16)\n",
    "      text = model.stt(data16)\n",
    "\n",
    "    # you can unmute the file printing of you want to \n",
    "      print(file)\n",
    "      print(text)\n",
    "      \n",
    "      print(len(file_list))\n",
    "      file_list.append(file)\n",
    "      text_list.append(text)\n",
    "      \n",
    "    except:\n",
    "        pass\n",
    "          \n",
    "\n",
    "\n",
    "  label  = pd.DataFrame(data=text_list, columns=[ \"transcript\"])\n",
    "  files = pd.DataFrame(data=file_list, columns=[\"file\"])\n",
    "\n",
    "  predicted_data = label.join(files)\n",
    "  return predicted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "22OGuVwjPPzm"
   },
   "outputs": [],
   "source": [
    "# sample on how to get prediction \n",
    "\n",
    "inp2 = \"/home/jupyter/CRP/Data/audio/Cleaned Audio files\"\n",
    "Deep_speech_prediction = Deep_Speech_stt(inp2)\n",
    "\n",
    "#removing .wav from the end of the file name\n",
    "Deep_speech_prediction['file'] = Deep_speech_prediction['file'].str.slice(0,-4,1)\n",
    "\n",
    "Deep_speech_prediction.to_csv('Deep_speech_prediction.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u8e6XltFR9TS"
   },
   "source": [
    "# 2. Google Speech-to-text\n",
    "- Google has [speech-to-text](https://cloud.google.com/speech-to-text/docs) as one of the Google Cloud services. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mFRQPrJVSrpK"
   },
   "source": [
    "2.1 Setup:\n",
    "\n",
    "**Upload Google Cloud Cred file**\n",
    "\n",
    "  - Have Google Cloud creds stored in a file named gc-creds.json, and upload it by running following code cell. See https://developers.google.com/accounts/docs/application-default-credentials for more details.\n",
    "\n",
    "  - This may reqire enabling third-party cookies. Check out https://colab.research.google.com/notebooks/io.ipynb for other alternatives.\n",
    "\n",
    "  - See the below link to learn how to create G-account Json:\n",
    "https://cloud.google.com/docs/authentication/getting-started\n",
    "\n",
    "  - More details are available [here](https://cloud.google.com/docs/authentication/production#auth-cloud-implicit-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-speech in /opt/conda/lib/python3.7/site-packages (1.3.2)\n",
      "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-speech) (1.16.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-speech) (2.23.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-speech) (2019.3)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-speech) (1.14.0)\n",
      "Requirement already satisfied: setuptools>=34.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-speech) (45.2.0.post20200209)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-speech) (1.51.0)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-speech) (1.11.2)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-speech) (3.11.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.8.2; extra == \"grpc\" in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-speech) (1.23.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-speech) (1.25.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-speech) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-speech) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-speech) (2019.11.28)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=0.4.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-speech) (4.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=0.4.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-speech) (3.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=0.4.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-speech) (0.2.7)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.7/site-packages (from rsa<4.1,>=3.1.4->google-auth<2.0dev,>=0.4.0->google-api-core[grpc]<2.0.0dev,>=1.14.0->google-cloud-speech) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "# !pip install google-cloud-speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7F4H1izlSpYI"
   },
   "outputs": [],
   "source": [
    "# upload the json file from the local machine\n",
    "# ------this only works with Google colab.------------ \n",
    "\n",
    "#-------------------------------------------------\n",
    "# from google.colab import files\n",
    "\n",
    "# uploaded = files.upload()\n",
    "\n",
    "# for fn in uploaded.keys():\n",
    "#   print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "#       name=fn, length=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cs-5KVfgTMmM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 jupyter jupyter 2341 Mar  8 14:23 /home/jupyter/CRP/gc_cred.json\n"
     ]
    }
   ],
   "source": [
    "# Set environment variable\n",
    "\n",
    "#Before running the code, upload the crednetial Json file to cloud and update its path\n",
    "import os\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/jupyter/CRP/gc_cred.json'\n",
    "\n",
    "!ls -l $GOOGLE_APPLICATION_CREDENTIALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HZ4GPKlkThFL"
   },
   "outputs": [],
   "source": [
    "#defining the google Speech to text function:\n",
    "\n",
    "from google.cloud import speech_v1\n",
    "from google.cloud.speech_v1 import enums\n",
    "\n",
    "def google_batch_stt(filename: str):\n",
    "    lang='en-US'\n",
    "    encoding= 'LINEAR16'\n",
    "\n",
    "    buffer, rate = read_wav_file(filename)\n",
    "    client = speech_v1.SpeechClient()\n",
    "\n",
    "    config = {\n",
    "        'language_code': lang,\n",
    "        'sample_rate_hertz': rate,\n",
    "        'encoding': enums.RecognitionConfig.AudioEncoding[encoding]\n",
    "    }\n",
    "\n",
    "    audio = {\n",
    "        'content': buffer\n",
    "    }\n",
    "\n",
    "    response = client.recognize(config, audio)\n",
    "    # For bigger audio file, the previous line can be replaced with following:\n",
    "    # operation = client.long_running_recognize(config, audio)\n",
    "    # response = operation.result()\n",
    "\n",
    "    for result in response.results:\n",
    "        # First alternative is the most probable result\n",
    "        alternative = result.alternatives[0]\n",
    "        return alternative.transcript\n",
    "\n",
    "\n",
    "# defining the function to process all the files in a folder:\n",
    "def Google_stt(inp):\n",
    "  file_list = []\n",
    "  text_list = []\n",
    "\n",
    "\n",
    "  for file in os.listdir(inp):\n",
    "      try:\n",
    "          current_file = os.path.join(inp, file)\n",
    "          text = google_batch_stt(current_file)\n",
    "\n",
    "          print(len(file_list))\n",
    "          print(file)\n",
    "          print(text)\n",
    "\n",
    "          file_list.append(file)\n",
    "          text_list.append(text)\n",
    "      except:\n",
    "        pass\n",
    "\n",
    "  label  = pd.DataFrame(data=text_list, columns=[ \"transcript\"])\n",
    "  files = pd.DataFrame(data=file_list, columns=[\"file\"])\n",
    "  predicted_data = label.join(files)\n",
    "  return predicted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9de3MvjvU_n1"
   },
   "outputs": [],
   "source": [
    "\n",
    "# here is an example of how to run the google API\n",
    "\n",
    "inp2 = \"/home/jupyter/CRP/Data/audio/test\" # insert the path here\n",
    "Google_prediction = Google_stt(inp2)\n",
    "\n",
    "#removing .wav from the end of the file name\n",
    "Google_prediction['file'] = Google_prediction['file'].str.slice(0,-4,1)\n",
    "\n",
    "Google_prediction.to_csv('Google_prediction.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the word Error rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to import a csv files and get the WER as compared with the labelled_data datafame\n",
    "def prepare_data(label, predict):\n",
    "    \n",
    "    labelled = pd.read_csv(label)\n",
    "    df_traslated = pd.read_csv(predict)\n",
    "    merged = pd.merge(left=df_traslated, right=labelled, how='left',left_on='file', right_on='file')\n",
    "    \n",
    "    # sometimes the model may not be able to predict some values and might return a empty list\n",
    "    # so we replace the empty list by nan\n",
    "    merged.replace('', np.nan, inplace=True)\n",
    "\n",
    "    #dropping the rows with nan\n",
    "    merged = merged.dropna()\n",
    "    \n",
    "    # creating the lists from the two dataframes\n",
    "    \n",
    "    predicted_list = merged['transcript_x'].tolist()\n",
    "    labelled_list = merged['transcript_y'].tolist()\n",
    "    \n",
    "    print(\"done preparing data\")\n",
    "    \n",
    "    return predicted_list, labelled_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wer_by_step(labelled_list, predicted_list, chunk_size):\n",
    "    \"\"\" Calculates the WER as the weighted average of WERs of smaller pieces of text.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    labelled_list : list of strings (each string is a sentence)\n",
    "        List of the original sentences (true labels)\n",
    "    predicted_list : list of strings (each string is a sentence)\n",
    "        List of the predicted sentences by the Speech Recognition Model chosen (predicted labels)\n",
    "    chunk_size : integer (>= 2)\n",
    "        Number of sentences per chunks on which we perform the WER calculation\n",
    "        The smaller the chunk_size, the fastest the WER will be computed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result : float64\n",
    "        Total WER\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    list_len = len(predicted_list)\n",
    "    total_word_count = 0\n",
    "    weighted_sum = 0\n",
    "    \n",
    "    print(\"Calculating WER step by step\")\n",
    "    \n",
    "    for i in range(0,int(floor(list_len/chunk_size))):\n",
    "        word_count = 0\n",
    "        labelled_list_short = labelled_list[i*chunk_size:min((i+1)*chunk_size-1,list_len-1)]\n",
    "        predicted_list_short = predicted_list[i*chunk_size:min((i+1)*chunk_size-1,list_len-1)]\n",
    "        error = wer(labelled_list_short, predicted_list_short)\n",
    "        \n",
    "        for items in labelled_list_short:\n",
    "            word_count += len(items.split())\n",
    "            \n",
    "        weighted_sum += error*word_count\n",
    "        total_word_count += word_count    \n",
    "        #print(\"The WER for step %s is %s with %s words \" % (i, error, word_count))\n",
    "        \n",
    "    result = weighted_sum/total_word_count\n",
    "    \n",
    "    print(\"The total WER is %s \" % result)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "#    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Finding the WER for Deep Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done preparing data\n",
      "Calculating WER step by step\n",
      "The total WER is 0.0799205097211694 \n",
      "--- 79.19299840927124 seconds ---\n"
     ]
    }
   ],
   "source": [
    "label = \"Deep_speech_prediction.csv\"\n",
    "predict = \"labelled.csv\"\n",
    "\n",
    "predicted_list, labelled_list = prepare_data(label, predict)\n",
    "wer_by_step(labelled_list, predicted_list, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Finding the WER for Google STT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done preparing data\n",
      "Calculating WER step by step\n",
      "The total WER is 0.12726113373755843 \n",
      "--- 78.4888288974762 seconds ---\n"
     ]
    }
   ],
   "source": [
    "label = \"Google_prediction.csv\"\n",
    "predict = \"labelled.csv\"\n",
    "\n",
    "predicted_list, labelled_list = prepare_data(label, predict)\n",
    "wer_by_step(labelled_list, predicted_list, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done preparing data\n",
      "Calculating WER step by step\n",
      "The total WER is 0.12975855435469466 \n",
      "--- 78.07587790489197 seconds ---\n"
     ]
    }
   ],
   "source": [
    "label = \"Google_prediction_sc.csv\"\n",
    "predict = \"labelled.csv\"\n",
    "\n",
    "predicted_list, labelled_list = prepare_data(label, predict)\n",
    "wer_by_step(labelled_list, predicted_list, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ohh Google why so smart!\n",
    "\n",
    "The reason for the lower WER for Google STT as compared to Mozilla Deep Speech is the patented technology by Google for processgin the transcribed text.\n",
    "\n",
    "For example Google Speech-to-Text returns “Mr.” instead of “Mister” and “7” for “seven”. \n",
    "\n",
    "Since all abbreviations and numbers are written out in the labeled dataset, the WER is higher for Google Speech-to-Text as it is supposed to be. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxilliary - Microsoft Azure\n",
    "\n",
    "In addition to the Deep Speech and Google STT models, we also tested the Microsoft Azure's cloud base off-the-sheve technology for speech to text. Since this is a paid service, we just used a demo version to create the pipeline and did not transcribed the entire Librispeech corpus.\n",
    "\n",
    "More info:\n",
    "\n",
    "Microsoft Azure [Speech Services](https://azure.microsoft.com/en-in/services/cognitive-services/speech-services/) have [Speech to Text](https://azure.microsoft.com/en-in/services/cognitive-services/speech-to-text/) service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting service credentials\n",
    "\n",
    "You can enable Speech service and find credentials for your account at [Microsoft Azure portal](https://portal.azure.com/). \n",
    "\n",
    "Just add Microsoft.CognitiveServicesSpeechServices to your azure account\n",
    "\n",
    "You can open a free account [here](https://azure.microsoft.com/en-in/free/ai/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-cognitiveservices-speech\n",
      "  Downloading azure_cognitiveservices_speech-1.10.0-cp37-cp37m-manylinux1_x86_64.whl (3.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.6 MB 4.5 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: azure-cognitiveservices-speech\n",
      "Successfully installed azure-cognitiveservices-speech-1.10.0\n"
     ]
    }
   ],
   "source": [
    "#Setup - Install azure speech package\n",
    "!pip install azure-cognitiveservices-speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set service credentials\n",
    "AZURE_SPEECH_KEY = '' # your Azure Speech Key here\n",
    "AZURE_SERVICE_REGION = '' # your azure serive region hee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp =  \"/home/jupyter/CRP/Data/audio/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the MIcrosoft Azure Speech to text function:\n",
    "\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "  \n",
    "\n",
    "def azure_batch_stt(filename: str):\n",
    "    lang='en-US'\n",
    "    encoding= 'LINEAR16'\n",
    "    \n",
    "    speech_config = speechsdk.SpeechConfig(\n",
    "        subscription=AZURE_SPEECH_KEY,\n",
    "        region=AZURE_SERVICE_REGION\n",
    "    )\n",
    "    audio_input = speechsdk.AudioConfig(filename=filename)\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(\n",
    "        speech_config=speech_config,\n",
    "        audio_config=audio_input\n",
    "    )\n",
    "    result = speech_recognizer.recognize_once()\n",
    "\n",
    "    return result.text if result.reason == speechsdk.ResultReason.RecognizedSpeech else None\n",
    "\n",
    "\n",
    "# defining the function to process all the files in a folder:\n",
    "def azure_stt(inp):\n",
    "  file_list = []\n",
    "  text_list = []\n",
    "\n",
    "\n",
    "  for file in os.listdir(inp):\n",
    "      try:\n",
    "          current_file = os.path.join(inp, file)\n",
    "          text =  azure_batch_stt(current_file)\n",
    "\n",
    "          print(len(file_list))\n",
    "          print(file)\n",
    "          print(text)\n",
    "\n",
    "          file_list.append(file)\n",
    "          text_list.append(text)\n",
    "      except:\n",
    "        pass\n",
    "\n",
    "  label  = pd.DataFrame(data=text_list, columns=[ \"transcript\"])\n",
    "  files = pd.DataFrame(data=file_list, columns=[\"file\"])\n",
    "  predicted_data = label.join(files)\n",
    "  return predicted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1272-128104-0001.wav\n",
      "Nor is Mr. Quilters manner less interesting than his matter.\n",
      "1\n",
      "1272-128104-0005.wav\n",
      "It is obviously unnecessary for us to point out how luminous these criticisms are, how delicate in expression.\n",
      "2\n",
      "1272-128104-0003.wav\n",
      "He has grave doubts whether Sir Frederick Leighton's work is really Greek after all, and can discover in it, but little of Rocky Ithaca.\n",
      "3\n",
      "1272-128104-0004.wav\n",
      "Lynels pictures are a sort of up guards and Adam paintings, and Masons Exquisite. It'll Zoras National as a Jingle poem.\n",
      "4\n",
      "1272-128104-0000.wav\n",
      "Mr quilter is the Apostle of the middle classes and we're glad to welcome his gospel.\n",
      "5\n",
      "1272-128104-0002.wav\n",
      "He tells us that at this festive season of the year with Christmas and roast beef looming before us, similes drawn from eating in its results occur most readily to the mind.\n"
     ]
    }
   ],
   "source": [
    "# here is an example of how to run the microsoft azure API\n",
    "\n",
    "inp2 = \"/home/jupyter/CRP/Data/audio/test\" # insert the path here\n",
    "azure_prediction = azure_stt(inp2)\n",
    "\n",
    "#removing .wav from the end of the file name\n",
    "azure_prediction['file'] = azure_prediction['file'].str.slice(0,-4,1)\n",
    "\n",
    "azure_prediction.to_csv('azure_prediction.csv', encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNclhyO4LJVd8aT8l1HREU2",
   "collapsed_sections": [],
   "name": "Untitled1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
